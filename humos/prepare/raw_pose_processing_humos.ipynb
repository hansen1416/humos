{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extract Poses from Amass Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T18:51:10.481390Z",
     "start_time": "2023-10-30T18:51:10.454550Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import contextlib\n",
    "\n",
    "\n",
    "from human_body_prior.tools.omni_tools import copy2cpu as c2c\n",
    "\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Please remember to download the following subdataset from AMASS website: https://amass.is.tue.mpg.de/download.php. Note only download the <u>SMPL+H G</u> data.\n",
    "* ACCD (ACCD)\n",
    "* HDM05 (MPI_HDM05)\n",
    "* TCDHands (TCD_handMocap)\n",
    "* SFU (SFU)\n",
    "* BMLmovi (BMLmovi)\n",
    "* CMU (CMU)\n",
    "* Mosh (MPI_mosh)\n",
    "* EKUT (EKUT)\n",
    "* KIT  (KIT)\n",
    "* Eyes_Janpan_Dataset (Eyes_Janpan_Dataset)\n",
    "* BMLhandball (BMLhandball)\n",
    "* Transitions (Transitions_mocap)\n",
    "* PosePrior (MPI_Limits)\n",
    "* HumanEva (HumanEva)\n",
    "* SSM (SSM_synced)\n",
    "* DFaust (DFaust_67)\n",
    "* TotalCapture (TotalCapture)\n",
    "* BMLrub (BioMotionLab_NTroje)\n",
    "\n",
    "### Unzip all datasets. In the bracket we give the name of the unzipped file folder. Please correct yours to the given names if they are not the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Place all files under the directory **.datasets/amass_data/**. The directory structure should look like the following:  \n",
    ".datasets/amass_data/  \n",
    ".datasets/amass_data/ACCAD/  \n",
    ".datasets/amass_data/BioMotionLab_NTroje/  \n",
    ".datasets/amass_data/BMLhandball/  \n",
    ".datasets/amass_data/BMLmovi/   \n",
    ".datasets/amass_data/CMU/  \n",
    ".datasets/amass_data/DFaust_67/  \n",
    ".datasets/amass_data/EKUT/  \n",
    ".datasets/amass_data/Eyes_Japan_Dataset/  \n",
    ".datasets/amass_data/HumanEva/  \n",
    ".datasets/amass_data/KIT/  \n",
    ".datasets/amass_data/MPI_HDM05/  \n",
    ".datasets/amass_data/MPI_Limits/  \n",
    ".datasets/amass_data/MPI_mosh/  \n",
    ".datasets/amass_data/SFU/  \n",
    ".datasets/amass_data/SSM_synced/  \n",
    ".datasets/amass_data/TCD_handMocap/  \n",
    ".datasets/amass_data/TotalCapture/  \n",
    ".datasets/amass_data/Transitions_mocap/  \n",
    "\n",
    "**Please make sure the file path are correct, otherwise it can not succeed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Choose the device to run the body model on.\n",
    "comp_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "\n",
    "male_bm_path = '../../body_models/smplh/male/model.npz'\n",
    "female_bm_path = '../../body_models/smplh/female/model.npz'\n",
    "\n",
    "num_betas = 10 # number of body parameters\n",
    "\n",
    "male_bm = BodyModel(bm_fname=male_bm_path, num_betas=num_betas).to(comp_device)\n",
    "faces = c2c(male_bm.f)\n",
    "\n",
    "female_bm = BodyModel(bm_fname=female_bm_path, num_betas=num_betas).to(comp_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paths = []\n",
    "folders = []\n",
    "dataset_names = []\n",
    "for root, dirs, files in os.walk('../../datasets/amass_data'):\n",
    "    # print(root, dirs, files)\n",
    "#     for folder in dirs:\n",
    "#         folders.append(os.path.join(root, folder))\n",
    "    # remove files if end with .tar.bz2 or .txt\n",
    "    files = [file for file in files if not file.endswith(\".tar.bz2\")]\n",
    "    folders.append(root)\n",
    "    for name in files:\n",
    "        if name.endswith(\".txt\"):\n",
    "            continue\n",
    "        dataset_name = root.split('/')[4]\n",
    "        if dataset_name not in dataset_names:\n",
    "            dataset_names.append(dataset_name)\n",
    "        paths.append(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eyes_Japan_Dataset', 'BioMotionLab_NTroje', 'BMLhandball', 'Transitions_mocap', 'SFU', 'BMLmovi', 'MPI_HDM05', 'TotalCapture', 'MPI_Limits', 'DFaust_67', 'TCD_handMocap', 'KIT', 'MPI_mosh', 'CMU', 'SSM_synced', 'HumanEva', 'ACCAD', 'EKUT']\n"
     ]
    }
   ],
   "source": [
    "save_root = '../../datasets/pose_data'\n",
    "save_folders = [folder.replace('../../datasets/amass_data', '../../datasets/pose_data') for folder in folders]\n",
    "for folder in save_folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "list_dataset = [\n",
    "    \"ACCAD\",\n",
    "    \"BioMotionLab_NTroje\",\n",
    "    \"BMLhandball\",\n",
    "    \"BMLmovi\",\n",
    "    \"CMU\",\n",
    "    \"DFaust_67\",\n",
    "    \"EKUT\",\n",
    "    \"Eyes_Japan_Dataset\",\n",
    "    \"HumanEva\",\n",
    "    \"KIT\",\n",
    "    \"MPI_HDM05\",\n",
    "    \"MPI_Limits\",\n",
    "    \"MPI_mosh\",\n",
    "    \"SFU\",\n",
    "    \"SSM_synced\",\n",
    "    \"TCD_handMocap\",\n",
    "    \"TotalCapture\",\n",
    "    \"Transitions_mocap\",\n",
    "]\n",
    "\n",
    "dataset_names = [n for n in dataset_names if n in list_dataset]\n",
    "print(dataset_names)\n",
    "\n",
    "group_path = [[path for path in paths if name in path] for name in dataset_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 18)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_names), len(group_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T18:53:14.751966Z",
     "start_time": "2023-10-30T18:53:14.636724Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trans_matrix = np.array([[1.0, 0.0, 0.0],\n",
    "                            [0.0, 0.0, 1.0],\n",
    "                            [0.0, 1.0, 0.0]]) # this is to make y up\n",
    "ex_fps = 20\n",
    "\n",
    "def amass_to_pose(src_path, save_path):\n",
    "    # Skip if outputs already exist\n",
    "    npz_path = save_path.replace('.npy', '.npz')\n",
    "    if os.path.exists(save_path) and os.path.exists(npz_path):\n",
    "        # still return fps if we can read it; otherwise fall back to 0\n",
    "        try:\n",
    "            bdata = np.load(src_path, allow_pickle=True)\n",
    "            return float(bdata.get('mocap_framerate', 0))\n",
    "        except Exception:\n",
    "            return 0\n",
    "        \n",
    "    bdata = np.load(src_path, allow_pickle=True)\n",
    "    fps = 0\n",
    "    \n",
    "    try:\n",
    "        fps = bdata['mocap_framerate']\n",
    "        frame_number = bdata['trans'].shape[0]\n",
    "    except:\n",
    "#         print(list(bdata.keys()))\n",
    "        return fps\n",
    "    \n",
    "    fId = 0 # frame id of the mocap sequence\n",
    "    pose_seq = []\n",
    "    verts_seq = []\n",
    "    if bdata['gender'] == 'male':\n",
    "        bm = male_bm\n",
    "    else:\n",
    "        bm = female_bm\n",
    "    down_sample = int(fps / ex_fps)\n",
    "    \n",
    "#     print(frame_number)\n",
    "#     print(fps)\n",
    "    data_seq = {\n",
    "        'root_orient': [],\n",
    "        'pose_body': [],\n",
    "        'pose_hand': [],\n",
    "        'betas': [], \n",
    "        'transl': [],\n",
    "        'gender': [], \n",
    "        'src_path': [],\n",
    "        'frameId': [],\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        count = 0\n",
    "        for fId in range(0, frame_number, down_sample):\n",
    "            root_orient = torch.Tensor(bdata['poses'][fId:fId+1, :3]).to(comp_device) # controls the global root orientation\n",
    "            pose_body = torch.Tensor(bdata['poses'][fId:fId+1, 3:66]).to(comp_device) # controls the body\n",
    "            pose_hand = torch.Tensor(bdata['poses'][fId:fId+1, 66:]).to(comp_device) # controls the finger articulation\n",
    "            betas = torch.Tensor(bdata['betas'][:10][np.newaxis]).to(comp_device) # controls the body shape\n",
    "            trans = torch.Tensor(bdata['trans'][fId:fId+1]).to(comp_device)\n",
    "            data_seq['root_orient'].append(root_orient)\n",
    "            data_seq['pose_body'].append(pose_body)\n",
    "            data_seq['pose_hand'].append(pose_hand)\n",
    "            data_seq['betas'].append(betas)\n",
    "            data_seq['transl'].append(trans)\n",
    "            data_seq['gender'].append(bdata['gender'])\n",
    "            data_seq['src_path'].append(src_path)\n",
    "            data_seq['frameId'].append(fId)\n",
    "            # Run SMPL forward\n",
    "            body = bm(pose_body=pose_body, pose_hand=pose_hand, betas=betas, root_orient=root_orient)\n",
    "            verts = body.v[0]+trans\n",
    "            joint_loc = body.Jtr[0] + trans\n",
    "            pose_seq.append(joint_loc.unsqueeze(0))\n",
    "            verts_seq.append(verts.unsqueeze(0))\n",
    "\n",
    "            # Visualize the meshes\n",
    "            # #############\n",
    "            # # save body vertices as objs in output folder\n",
    "            # out_folder = './meshes/'\n",
    "            # os.makedirs(out_folder, exist_ok=True)\n",
    "            # # save the mesh as an obj file\n",
    "            # import trimesh\n",
    "            # verts = torch.bmm(verts.unsqueeze(0), torch.FloatTensor(trans_matrix).unsqueeze(0))[0]\n",
    "            # body_mesh = trimesh.Trimesh(vertices=c2c(verts), faces=c2c(bm.f), vertex_colors=np.tile([255, 200, 200, 255], (6890, 1)))\n",
    "            # body_mesh.export(out_folder + f'body_mesh_{count:04d}.obj')\n",
    "            # print(out_folder + f'body_mesh_{count:04d}.obj')\n",
    "            # count += 1\n",
    "            # ###############\n",
    "\n",
    "    pose_seq = torch.cat(pose_seq, dim=0)\n",
    "    pose_seq_np = pose_seq.detach().cpu().numpy()\n",
    "\n",
    "    # Make Y up instead of Z up\n",
    "    pose_seq_np_n = np.dot(pose_seq_np, trans_matrix)\n",
    "    \n",
    "    np.save(save_path, pose_seq_np_n)\n",
    "    \n",
    "    # concatenate all keys in dataseq\n",
    "    for k, v in data_seq.items():\n",
    "        if isinstance(v[0], torch.Tensor):\n",
    "            data_seq[k] = torch.cat(v, dim=0).detach().cpu().numpy()\n",
    "    np.savez(save_path.replace('.npy', '.npz'), **data_seq)\n",
    "    return fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "group_path = group_path\n",
    "all_count = sum([len(paths) for paths in group_path])\n",
    "cur_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14055\n"
     ]
    }
   ],
   "source": [
    "print(all_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This will take a few hours for all datasets, here we take one dataset as an example\n",
    "\n",
    "To accelerate the process, you could run multiple scripts like this at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: Eyes_Japan_Dataset: 100%|██████████| 750/750 [00:00<00:00, 11580.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.0\n",
      "Processed / All (fps 120): 12058/14055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: BioMotionLab_NTroje: 100%|██████████| 3061/3061 [00:00<00:00, 11321.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.0\n",
      "Processed / All (fps 120): 15119/14055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: BMLhandball: 100%|██████████| 659/659 [00:00<00:00, 9971.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.0\n",
      "Processed / All (fps 120): 15778/14055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: Transitions_mocap: 100%|██████████| 110/110 [00:00<00:00, 5053.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.0\n",
      "Processed / All (fps 120): 15888/14055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: SFU: 100%|██████████| 44/44 [00:00<00:00, 2919.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.0\n",
      "Processed / All (fps 120): 15932/14055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: BMLmovi: 100%|██████████| 1887/1887 [00:00<00:00, 11463.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.0\n",
      "Processed / All (fps 120): 17819/14055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: MPI_HDM05: 100%|██████████| 215/215 [00:00<00:00, 6945.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.0\n",
      "Processed / All (fps 120): 18034/14055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: TotalCapture: 100%|██████████| 37/37 [00:00<00:00, 2951.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.0\n",
      "Processed / All (fps 60): 18071/14055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: MPI_Limits: 100%|██████████| 35/35 [00:00<00:00, 2716.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.0\n",
      "Processed / All (fps 120): 18106/14055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: DFaust_67: 100%|██████████| 139/139 [00:00<00:00, 5414.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.0\n",
      "Processed / All (fps 60): 18245/14055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: TCD_handMocap: 100%|██████████| 62/62 [00:00<00:00, 3706.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.0\n",
      "Processed / All (fps 120): 18307/14055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: KIT: 100%|██████████| 4232/4232 [00:00<00:00, 11671.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "Processed / All (fps 100): 22539/14055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: MPI_mosh: 100%|██████████| 77/77 [00:00<00:00, 3503.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "Processed / All (fps 100): 22616/14055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: CMU:   2%|▏         | 47/2088 [00:05<04:06,  8.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../datasets/amass_data\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../datasets/pose_data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m save_path[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnpy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 11\u001b[0m     fps \u001b[38;5;241m=\u001b[39m \u001b[43mamass_to_pose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(fps)\n\u001b[1;32m     13\u001b[0m cur_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(paths)\n",
      "Cell \u001b[0;32mIn[31], line 65\u001b[0m, in \u001b[0;36mamass_to_pose\u001b[0;34m(src_path, save_path)\u001b[0m\n\u001b[1;32m     63\u001b[0m data_seq[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframeId\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(fId)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Run SMPL forward\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m body \u001b[38;5;241m=\u001b[39m \u001b[43mbm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpose_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpose_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose_hand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpose_hand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbetas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_orient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_orient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m verts \u001b[38;5;241m=\u001b[39m body\u001b[38;5;241m.\u001b[39mv[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39mtrans\n\u001b[1;32m     67\u001b[0m joint_loc \u001b[38;5;241m=\u001b[39m body\u001b[38;5;241m.\u001b[39mJtr[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m trans\n",
      "File \u001b[0;32m~/anaconda3/envs/humos_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/humos_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/humos_p310/lib/python3.10/site-packages/human_body_prior/body_model/body_model.py:257\u001b[0m, in \u001b[0;36mBodyModel.forward\u001b[0;34m(self, root_orient, pose_body, pose_hand, pose_jaw, pose_eye, betas, trans, dmpls, expression, v_template, joints, v_shaped, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     shape_components \u001b[38;5;241m=\u001b[39m betas\n\u001b[1;32m    255\u001b[0m     shapedirs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshapedirs\n\u001b[0;32m--> 257\u001b[0m verts, Jtr \u001b[38;5;241m=\u001b[39m \u001b[43mlbs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_pose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mshapedirs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshapedirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposedirs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposedirs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mJ_regressor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJ_regressor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkintree_table\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlbs_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_shaped\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv_shaped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m Jtr \u001b[38;5;241m=\u001b[39m Jtr \u001b[38;5;241m+\u001b[39m trans\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    264\u001b[0m verts \u001b[38;5;241m=\u001b[39m verts \u001b[38;5;241m+\u001b[39m trans\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/humos_p310/lib/python3.10/site-packages/human_body_prior/body_model/lbs.py:237\u001b[0m, in \u001b[0;36mlbs\u001b[0;34m(betas, pose, v_template, shapedirs, posedirs, J_regressor, parents, lbs_weights, joints, pose2rot, v_shaped, dtype)\u001b[0m\n\u001b[1;32m    235\u001b[0m v_posed \u001b[38;5;241m=\u001b[39m pose_offsets \u001b[38;5;241m+\u001b[39m v_shaped\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# 4. Get the global joint location\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m J_transformed, A \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_rigid_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrot_mats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mJ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# 5. Do skinning:\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# W is N x V x (J + 1)\u001b[39;00m\n\u001b[1;32m    241\u001b[0m W \u001b[38;5;241m=\u001b[39m lbs_weights\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand([batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/humos_p310/lib/python3.10/site-packages/human_body_prior/body_model/lbs.py:387\u001b[0m, in \u001b[0;36mbatch_rigid_transform\u001b[0;34m(rot_mats, joints, parents, dtype)\u001b[0m\n\u001b[1;32m    383\u001b[0m transform_chain \u001b[38;5;241m=\u001b[39m [transforms_mat[:, \u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, parents\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# Subtract the joint location at the rest pose\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# No need for rotation, since it's identity when at rest\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     curr_res \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform_chain\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparents\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtransforms_mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     transform_chain\u001b[38;5;241m.\u001b[39mappend(curr_res)\n\u001b[1;32m    391\u001b[0m transforms \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(transform_chain, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "for paths in group_path:\n",
    "    dataset_name = paths[0].split('/')[4]\n",
    "\n",
    "    pbar = tqdm(paths)\n",
    "    pbar.set_description('Processing: %s'%dataset_name)\n",
    "    fps = 0\n",
    "    for path in pbar:\n",
    "        save_path = path.replace('../../datasets/amass_data', '../../datasets/pose_data')\n",
    "        save_path = save_path[:-3] + 'npy'\n",
    "        fps = amass_to_pose(path, save_path)\n",
    "    print(fps)\n",
    "    cur_count += len(paths)\n",
    "    print('Processed / All (fps %d): %d/%d'% (fps, cur_count, all_count) )\n",
    "    time.sleep(0.5)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The above code will extract poses from **AMASS** dataset, and put them under directory **\"../../datasets/pose_data\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humos_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
